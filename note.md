1. Keyboard input is innately handled by the Pokemon.gb ROM simulator. The interactive thing is rather a bug (where the keyboard input can NOT be turned off, and not becuase they walk an extra mile to enable keyboard control here)
2. pl.py file proves that the .gb simulator has built-in keyboard interaction mechanism
3. Watching the recorded trained behavior of the RL agent, and I realize some silly behaviors, the agent seems to be unable to recognize a simple fact: If you have lower grades than the opponent, then you simply can not win the game, and losing the game means you will most likely dies. Such common sense seems to be completely ignored by the agent. I wonder why is this the case. 
4. GPT-4 can be leveraged to design the reward function, as well as some of the useful high-level strategy & actions. But there is a long way to go and blanks to fill it. 
5. Lets' try with GPT-designed reward functions, first. We can start with a simple prompt to GPT4 on web here. What do I provide to it? Plus, there should be approach by which I observe how my trained agents perform under the Pokemon environment, otherwise I am forced to look at those non-sense stat, which is rather useless.
6. all of a suddent the 'go to definition' stuff stops working?
7. The drawback of Eureka is, one typically needs to provide information of the environment in the form of the 'source code', and the reward function will also be provided in 'source code' format. This becomes unreasonable for emulator based Pokemon game environment -- it is not in python, and i do not think I can get the source code if I want it. There should be some other ways?
8 While RL Agent learns on the 'black-box-style' neural connection, a LLM-RL agent should learns on the 'rules' in a high level fashion, something like the prompt: 'based on the previous experience {prev_experience}, your designed reward function {prev_function} suffers from {...}, please update your experience and the reward function' (except that this should happens in an automatic ways, hopefully with less human feedback? Even with human feedback, this will be very much acceptable here)
9. Since the observation is not directly related to 'entity' status, and things like pathfinding, battle can not be directly coded out, we can instead use many 'sub-goals' to built up a 'skill-library' first, for instance
  -- reward: level up & stay alive | skill: encounter wild pokemon and level up | log: speed of leveling up
  -- reward: explore & stay alive | skill: explore unseen terrain | log: ....
Then the high-level capacity of the LLM shall be leveraged to do explicit planning, such as 'oh, the gym boss has pokemon of level 29, which is higher than mine, so I've better level up before I fight him, turn towards the trained LEVELUP gadget !'
10. LLM is in charge of high-level planning, using tools such as 'hit with what skill', 'move towards grass (?)', 'win with high health left', and the reasoning will be carried out by the LLM (only the easiest ones). But it really boils down to the basic stuff -- How to get those properties from the environment itself?
11. Somehow the habit behavior is a short-cut to all the goal-directed learning & evaluating process. It compresses the complicated evaluation & prediction process down to a sinle dictionary-like memory circuit. And once triggered, it would just follow its route, as long as no 'Surprise' is given. Check out the Active Learning and Inference paper about a theoretical description of such concept.
12. 2increment -- contextual reward design so that we enables 'trainer', 'explorer', 'battler' to work together (since in these scenarios, the reward function will have different shape) | The question is why does the agent learns to 'avoid' fighting, and during battle, it tries intead to capture a pokemon instead of killing one and level up | It feels like the task of 'leveling up' is essentially too far-fetched (!) for the agent to figure out from scratch ... The chance of dying is just too much higher than the chance of winning enough battle so that it would level up. 
13. Some human demonstration can be applied here to train the Policy model I suppose (?) One of the issue could be the lack of 'end battle, beat opponent' flag in the program. 
14. The LLM agent simply need to get feedback, and refine on its performance, hopefully in an automatic fashion... 
15. Two Question -- How to align Image with Textual Description, How to align Textual Strategy with Action control, without much of the annotations... Or, can we simply use a blackbox NN to do the dual alignment, based on the reward design...
16. How do you 'GO-BACK' in a map, this can be very useful, just like a robot cleaner, my agent needs to return to the hospital and heal it self. This better not be deliberately programed (?) In the embodied agent lecture, the first author definitely mentioned something about going back with a simple CNN...
17. Do no know why (maybe the training routine is never completely finished), but the agent stat is not the model checkpoint correlated, it is a much more historical record and should not be considered. 
